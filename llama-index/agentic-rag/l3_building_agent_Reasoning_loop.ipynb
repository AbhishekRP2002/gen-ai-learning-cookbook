{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install helper nest_asyncio python-dotenv -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "# print(nodes[0].get_content(metadata_mode=\"all\"))\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "    \n",
    "\n",
    "vector_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    fn=vector_query\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful if you want to get a summary of MetaGPT\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The agent roles in MetaGPT include Product Manager, Architect, Project Manager, Engineer, and QA Engineer. The Product Manager is responsible for creating the Product Requirement Document and conducting competitive analysis. The Architect is in charge of designing the system architecture, interface definitions, and technical specifications. The Project Manager breaks down the project into tasks and assigns them to Engineers. Engineers develop the code based on the provided specifications. The QA Engineer reviews the code, creates unit tests, and ensures the software's quality. Each role is essential in the collaborative software development process within MetaGPT.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"how agents communicate in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Agents in MetaGPT communicate through structured interfaces and a shared message pool. They publish structured messages and can subscribe to relevant ones based on their role profiles. This method allows for direct and transparent information exchange. Agents use role-specific interests to extract task-related information, ensuring they receive only relevant details and avoid distractions. This structured communication mechanism enhances collaboration and workflow efficiency among agents in MetaGPT.\n",
      "=== LLM Response ===\n",
      "In MetaGPT, the agent roles include:\n",
      "\n",
      "1. **Product Manager**: Responsible for creating the Product Requirement Document (PRD) and conducting competitive analysis.\n",
      "2. **Architect**: In charge of designing the system architecture, interface definitions, and technical specifications.\n",
      "3. **Project Manager**: Breaks down the project into tasks and assigns them to Engineers.\n",
      "4. **Engineer**: Develops the code based on the provided specifications.\n",
      "5. **QA Engineer**: Reviews the code, creates unit tests, and ensures the software's quality.\n",
      "\n",
      "These roles are essential in the collaborative software development process within MetaGPT.\n",
      "\n",
      "As for communication, agents in MetaGPT communicate through structured interfaces and a shared message pool. They publish and subscribe to structured messages relevant to their role profiles, allowing for direct and transparent information exchange. Agents use role-specific interests to filter and extract task-related information, ensuring they receive only the details pertinent to their tasks and avoid distractions. This structured communication mechanism is designed to enhance collaboration and workflow efficiency among the different roles in MetaGPT.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation datasets used,and also mention the results obtained for one of the datasets.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"evaluation datasets used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets used in MetaGPT include HumanEval, MBPP, and a self-generated SoftwareDev dataset. The HumanEval dataset consists of 164 handwritten programming tasks, while the MBPP dataset comprises 427 Python tasks. The SoftwareDev dataset includes 70 representative software development tasks covering various scopes such as mini-games, image processing algorithms, and data visualization. These datasets were utilized to evaluate the performance of MetaGPT in code generation tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"results obtained for one of the evaluation datasets in MetaGPT\", \"page_numbers\": []}\n",
      "=== Function Output ===\n",
      "The results obtained for one of the evaluation datasets in MetaGPT show that it achieved an average score of 3.9, surpassing the score of ChatDev, which was based on the Chat chain. Additionally, when compared to general intelligent algorithms like AutoGPT, MetaGPT displayed significantly higher performance, particularly in generating executable code.\n",
      "=== LLM Response ===\n",
      "The evaluation datasets used in MetaGPT are:\n",
      "\n",
      "1. **HumanEval**: This dataset includes 164 handwritten programming tasks.\n",
      "2. **MBPP**: Comprising 427 Python programming tasks.\n",
      "3. **SoftwareDev**: A self-generated dataset with 70 representative software development tasks, covering various scopes such as mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "These datasets were used to assess the performance of MetaGPT in code generation tasks.\n",
      "\n",
      "Regarding the results obtained for one of the datasets, MetaGPT achieved an average score of 3.9. This score surpassed that of ChatDev, which was based on the Chat chain. Moreover, when compared to general intelligent algorithms like AutoGPT, MetaGPT demonstrated significantly higher performance, especially in generating executable code.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me about the evaluation datasets used,\"\n",
    "    \"and also mention the results obtained for one of the datasets.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
